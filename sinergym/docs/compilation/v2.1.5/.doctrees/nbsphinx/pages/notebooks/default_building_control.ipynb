{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Default building control setting up an empty action interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When you want to run a simulation with all the default building controllers (specified in the IDF file). You can directly use the **EnergyPlus simulation engine**. For example, placing us in the workspace of the container would be to run the following:\n",
    "\n",
    "```bash\n",
    "$ energyplus -w sinergym/data/weather/USA_PA_Pittsburgh-Allegheny.County.AP.725205_TMY3.epw sinergym/data/buildings/5ZoneAutoDXVAV.idf \n",
    "```\n",
    "\n",
    "However, doing so without our framework has some **disadvantages**. You will have EnergyPlus default output and will not have some **added outputs** such as our wrapper logger that monitors all interactions with the environment. The IDFs have a default *location* and *designday*, which Sinergym changes automatically depending on the specified weather, so you would have to **adjust it** before using the simulator manually. Finally, the *runperiod* of the IDFs are set to one episode for DRL, which means that as the IDFs stand you can only simulate one year. So, you would also have to modify the *runperiod* **manually** in the IDF before starting the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, our recommended proposal is setting up an **empty action interface** in an environment with Sinergym. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 12:14:14,314] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Updating idf ExternalInterface object if it is not present...\n",
      "[2022-12-01 12:14:14,315] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Updating idf Site:Location and SizingPeriod:DesignDay(s) to weather and ddy file...\n",
      "[2022-12-01 12:14:14,317] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Updating idf OutPut:Variable and variables XML tree model for BVCTB connection.\n",
      "[2022-12-01 12:14:14,318] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Setting up extra configuration in building model if exists...\n",
      "[2022-12-01 12:14:14,318] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Setting up action definition in building model if exists...\n",
      "[2022-12-01 12:14:14,320] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
      "[2022-12-01 12:14:14,341] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res1/Eplus-env-sub_run1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  -0.008417052432114895 {'timestep': 1, 'time_elapsed': 900, 'year': 1991, 'month': 1, 'day': 1, 'hour': 0, 'total_power': 168.3410486422979, 'total_power_no_units': -0.01683410486422979, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [22.41070439945542, 19.27041626118706, 21.84918155106144, 21.88116986202595, 22.74532738560474, 21.73841316494982, 19.38460288121428, 19.66372873926693, 21.24314170888169, 20.12321836432302, 19.67730731535304, 19.88721275822578, 21.17223251340036, 20.30793764011797, 20.20143741435643, 20.4627284531045, 22.03096300421944, 20.93365898954337], 'out_temperature': 4.4, 'action_': []}\n",
      "Reward:  -1482.85953514518 {'timestep': 2976, 'time_elapsed': 2678400, 'year': 1991, 'month': 2, 'day': 1, 'hour': 0, 'total_power': 249.4442211987932, 'total_power_no_units': -0.024944422119879323, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [23.69669852361374, 22.25313699035975, 24.31323981198721, 23.87141679758788, 25.11090427756838, 24.56241622682827, 22.9404356024169, 22.94282444325588, 23.67194978294257, 23.12528250474749, 22.02675393355748, 22.02881519606414, 22.7323170557345, 22.27057664160117, 23.38432687212007, 23.31131614661928, 24.12704060142421, 23.5512781399194], 'out_temperature': 12.1, 'action_': []}\n",
      "Reward:  -2254.712265755012 {'timestep': 5664, 'time_elapsed': 5097600, 'year': 1991, 'month': 3, 'day': 1, 'hour': 0, 'total_power': 249.4442211987932, 'total_power_no_units': -0.024944422119879323, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [23.6570166669069, 22.06842055195615, 24.19611060077995, 23.76257909194055, 25.03948097340511, 24.44816667157681, 22.68246103494577, 22.74943691501284, 23.57724440341409, 23.04252087505039, 21.82097230898479, 21.89365754884302, 22.57236745458363, 22.23540331299291, 23.15797185610974, 23.15254710004311, 24.07075284558289, 23.49970749537007], 'out_temperature': 11.9, 'action_': []}\n",
      "Reward:  -3618.41549254088 {'timestep': 8640, 'time_elapsed': 7776000, 'year': 1991, 'month': 4, 'day': 1, 'hour': 0, 'total_power': 201.198977940785, 'total_power_no_units': -0.020119897794078502, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [22.95177229570344, 22.03365818920899, 23.60757560084356, 23.01807134766616, 24.02808474031944, 23.82190812609193, 22.48682887050774, 22.88794828178202, 23.59810935786398, 23.46778150537447, 21.44257972669456, 21.79284879509999, 22.43562436709753, 22.41247285833814, 22.62715643472103, 23.03611257878451, 23.79267736363525, 23.65243564408237], 'out_temperature': 15.3, 'action_': []}\n",
      "Reward:  -6482.27876446466 {'timestep': 11520, 'time_elapsed': 10368000, 'year': 1991, 'month': 5, 'day': 1, 'hour': 0, 'total_power': 201.198977940785, 'total_power_no_units': -0.020119897794078502, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [23.86509413798045, 24.1071046471536, 25.27509765486189, 24.55675636953588, 25.65216773896621, 25.5571280438139, 24.79071520939431, 24.83977394603949, 24.98455644317614, 25.15279122447957, 23.20765409229273, 23.24402610262762, 23.43118400765561, 23.60430118197448, 24.86909845458321, 24.87644643022476, 25.06484591156406, 25.24300908399611], 'out_temperature': 19.4, 'action_': []}\n",
      "Reward:  -9982.991284981603 {'timestep': 14496, 'time_elapsed': 13046400, 'year': 1991, 'month': 6, 'day': 1, 'hour': 0, 'total_power': 168.6090046831275, 'total_power_no_units': -0.01686090046831275, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [23.94899380246081, 25.06360575139866, 25.64043835708107, 24.90053413764347, 25.77775409162826, 25.85491440084278, 25.84413809380008, 25.84852169014866, 25.81155249007516, 26.2474442023517, 24.08359041763356, 24.03184496587623, 24.06114037003215, 24.49805825736507, 25.80319610787662, 25.78635379853651, 25.76633279008873, 26.23300358565249], 'out_temperature': 26.0, 'action_': []}\n",
      "Reward:  -15197.775861760021 {'timestep': 17376, 'time_elapsed': 15638400, 'year': 1991, 'month': 7, 'day': 1, 'hour': 0, 'total_power': 6430.25476366733, 'total_power_no_units': -0.643025476366733, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [25.30362458961184, 26.08749938089204, 26.47728046995123, 25.84848610748885, 25.35441535664673, 26.23674788167068, 26.83684923572157, 26.48953443144097, 26.70569108217305, 26.92570822385427, 25.95453942245817, 25.9310852109356, 25.90761065616391, 26.54695943183036, 26.02379180782778, 25.09140023176932, 25.79379237233488, 25.62275516349538], 'out_temperature': 28.1, 'action_': []}\n",
      "Reward:  -20393.055482672924 {'timestep': 20352, 'time_elapsed': 18316800, 'year': 1991, 'month': 8, 'day': 1, 'hour': 0, 'total_power': 201.198977940785, 'total_power_no_units': -0.020119897794078502, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [25.40730428893415, 26.2081195882295, 25.94397819959719, 25.63057921118531, 26.16215148190642, 26.34597161854728, 26.68084155872953, 26.79261102276273, 26.80021185275091, 26.92354348202196, 25.72571798349205, 25.7933442496152, 25.89388375583001, 25.94708364107272, 26.52064693594591, 26.62639383402464, 26.64389562611166, 26.78208493279325], 'out_temperature': 29.6, 'action_': []}\n",
      "Reward:  -25435.235586329356 {'timestep': 23328, 'time_elapsed': 20995200, 'year': 1991, 'month': 9, 'day': 1, 'hour': 0, 'total_power': 168.6090046831275, 'total_power_no_units': -0.01686090046831275, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [25.29888599656579, 25.60513463360622, 25.6502371961184, 25.26243009865059, 25.83511693216035, 26.01650759969682, 26.1219715833017, 26.193129951486, 26.25984927684709, 26.246756226096, 25.16961935329535, 25.2292706821575, 25.31058065863118, 25.30679185978435, 26.00162377086149, 26.07019493415782, 26.14488073433873, 26.15733356377928], 'out_temperature': 26.3, 'action_': []}\n",
      "Reward:  -29667.280742048788 {'timestep': 26208, 'time_elapsed': 23587200, 'year': 1991, 'month': 10, 'day': 1, 'hour': 0, 'total_power': 201.198977940785, 'total_power_no_units': -0.020119897794078502, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [25.31232078408934, 25.15531893926972, 25.62243610547647, 25.3355501078475, 25.99617019496714, 25.90925164972085, 25.78478083621939, 25.76648417323338, 25.94552529042894, 25.81660183265429, 25.15665229426293, 25.11984467906358, 25.34128949509953, 25.20504796353065, 25.88586123209223, 25.84549591879043, 26.0480387071551, 25.92364136523652], 'out_temperature': 25.2, 'action_': []}\n",
      "Reward:  -32266.669648540534 {'timestep': 29184, 'time_elapsed': 26265600, 'year': 1991, 'month': 11, 'day': 1, 'hour': 0, 'total_power': 201.198977940785, 'total_power_no_units': -0.020119897794078502, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [23.95342186248593, 22.79017615251069, 24.61576801735736, 24.21904412959735, 25.23337652641756, 24.70199079621272, 23.43858732417969, 23.50085261539548, 24.30684175112909, 23.68506692334094, 22.61997561531062, 22.64591022418004, 23.43849129649558, 22.89730314513248, 23.86935225127617, 23.8610257810776, 24.76415193815065, 24.1030080798225], 'out_temperature': 17.1, 'action_': []}\n",
      "Reward:  -33693.4200577215 {'timestep': 32064, 'time_elapsed': 28857600, 'year': 1991, 'month': 12, 'day': 1, 'hour': 0, 'total_power': 168.6090046831275, 'total_power_no_units': -0.01686090046831275, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [22.67471404763066, 20.76426059004519, 23.20781447906961, 22.8565960165495, 23.84452889598035, 23.03748347863898, 21.30287781156127, 21.44280752813544, 22.36536796313164, 21.55357872462707, 20.79446517569299, 20.90682538688919, 21.72661725183894, 21.03753160386922, 21.91255021343735, 22.01250206066998, 22.95371613155545, 22.12462597231874], 'out_temperature': 9.3, 'action_': []}\n",
      "Reward:  -34861.66962269026 {'timestep': 35040, 'time_elapsed': 31536000, 'year': 1992, 'month': 1, 'day': 1, 'hour': 0, 'total_power': 249.4442211987932, 'total_power_no_units': -0.024944422119879323, 'comfort_penalty': -0.0, 'abs_comfort': 0.0, 'temperatures': [23.49202797924699, 21.91834253143812, 23.6676225575917, 23.40668187489388, 24.55052344518836, 23.93795083204492, 22.60466670710505, 22.58536199071002, 23.28750630729937, 22.6461216699741, 22.06515682888017, 22.06046316147131, 22.57842146719483, 22.13899733853399, 23.14064407629781, 23.0206640579445, 23.80239869708953, 23.11389817828658], 'out_temperature': 16.0, 'action_': []}\n",
      "Episode  0 Mean reward:  -0.9949106627480553 Cumulative reward:  -34861.66962269026\n",
      "[2022-12-01 12:14:45,665] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:EnergyPlus simulation closed successfully. \n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import sinergym\n",
    "from sinergym.utils.wrappers import LoggerWrapper\n",
    "\n",
    "env = gym.make(\n",
    "    'Eplus-office-hot-continuous-v1',\n",
    "    action_variables=[],\n",
    "    action_space=gym.spaces.Box(\n",
    "        low=0,\n",
    "        high=0,\n",
    "        shape=(0,)),\n",
    "    action_definition=None)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "for i in range(1):\n",
    "    obs = env.reset()\n",
    "    rewards = []\n",
    "    done = False\n",
    "    current_month = 0\n",
    "    while not done:\n",
    "        a = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(a)\n",
    "        rewards.append(reward)\n",
    "        if info['month'] != current_month:  # display results every month\n",
    "            current_month = info['month']\n",
    "            print('Reward: ', sum(rewards), info)\n",
    "    print(\n",
    "        'Episode ',\n",
    "        i,\n",
    "        'Mean reward: ',\n",
    "        np.mean(rewards),\n",
    "        'Cumulative reward: ',\n",
    "        sum(rewards))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a default environment is loaded, but the space and definition of the default action **is replaced with an empty one**, Sinergym takes care of making the necessary changes in the background. Then, the random agent implemented send **empty actions ([])**.\n",
    "\n",
    "The advantages are that you can **combine the weathers with the buildings** as you want, Sinergym will take care of adapting everything automatically (you don't have the disadvantages of before), you can run in a single experiment as many years as you want, with our loggers. When you set an empty action interface, Sinergym preserves the default actuators that the IDF comes with (these can be more or less sophisticated depending on the definition of the building in the IDF)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
