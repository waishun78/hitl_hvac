# Prerequisites to understanding the Isolation Forest (Anomaly Detection Ensemble Unsupervised Machine Learning (ML) algorithm) and Isolation
# Trees (Anomaly Detection Unsupervised Machine Learning (ML) algorithm):
# What is a Decision Tree (Classification and Regression Supervised Machine Learning (ML) algorithm)?
# Decision Tree is a type of Supervised Classification and Regression Machine Learning (ML) algorithm that shows a clear pathway to a 
# decision. In terms of data analytics, it is a type of algorithm that includes conditional 'control' statements to classify data. A 
# Decision Tree starts at a single point (or 'node') which then branches (or 'splits') in two or more directions. Each branch offers
# different possible outcomes, incorporating a variety of decisions and chance events until a final outcome is achieved. When shown 
# visually, their appearance is tree-likeâ€¦ hence the name!

# A Decision Tree makes predictions by traversing from the root node to a leaf node based on the input independent variables/features of the data instance. 
# Each node in the tree represents a decision based on one of the input independent variables/features, and each branch represents the outcome of that 
# decision. 
# Visualisation of how a Decision Tree ML algorithm makes a prediction (in a context):
#               Weather
#          /      |      \
#      Sunny     Rainy  Overcast
#     /     \     No       Yes
# Is Sunny Not Sunny
#     |       No 
#   Temp  
#   /  \  
# Hot  Mild              
# No    No              



# What is a Random Forest (Classification and Regression Ensemble Supervised Machine Learning (ML) algorithm)?
# Random Forest is a type of Supervised Classification and Regression Ensemble Machine Learning (ML) algorithm, that consists of
# multiple Decision Trees ML algorithms which are each trained using a different subset of the dataset and merges them (via Majority 
# Voting, but other types of aggregation methods are possible too) to get a more accurate and stable prediction. It enhances the 
# performance of Decision Trees ML algorithms by reducing overfitting and increasing accuracy.
# Visualisation of how a Random Forest Ensemble ML algorithm makes a prediction (in a context):
# Decision Tree 1 of predicting if someone will play tennis: (logic here is a bit weirdddddddddddd, cuz generated by ChatGPT, but I think it gets the rough idea across on what a Random Forest is)
#             Weather
#            /       \
#        Sunny      Rainy
#        /   \       /   \
#     Temp     Yes  Temp   No
#     /   \        /   \
#   Hot  Mild   Mild  Mild
#   No    Yes    Yes   Yes

# Decision Tree 2:
#             Weather
#            /       \
#       Overcast    Rainy
#          |         /   \
#         Yes     Temp   No
#                  /   \
#                Hot  Mild
#                 No   Yes

# Decision Tree 3:
#             Weather
#            /       \
#       Overcast    Sunny
#          |         /   \
#         Yes     Temp   Temp
#                 /   \    
#                 Hot  Mild
#                 No   Yes


# ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


# What is a Isolation Tree (Anomaly Detection Unsupervised Machine Learning (ML) algorithm)?
# While a Decision Tree aim to divide a bunch of data into homogenous group (groups that contains data of the same type), an Isolation Tree
# aim to divide a bunch of data into individual data (where each 'group' only contains one data).

# The idea of detecting anomaly using an Isolation Tree is that 'Anomalies tend to have shorter paths because they are easier to separate 
# from the rest of the data compared to normal data', since anomalous data will be further away from the main cluster of normal data in a 
# dataset.

# Step by step of how to Isolation Tree ML algorithm detects anomalies:
# 1. Isolation Trees (iTrees):
# - Each iTree is constructed by randomly selecting a feature and then randomly selecting a split value for that feature.
# - This process is repeated recursively until all points are isolated, or a maximum tree height is reached.

# 2. Isolation Path Length:
# - The path length from the root of the tree to the node where the data point is isolated is measured.
# - Anomalies tend to have shorter paths because they are easier to separate from the rest of the data.

# Visualisation of how a 'shorter path length' look like:
#             feature1
#            /       \
#       feature2   feature3
#         Yes       /   \
#              feature4   feature5
#                 No        Yes

# The prediction of the feature2 has a shorter path length that the predictions of feature4 and feature5.



# What is a Isolation Forest (Anomaly Detection Ensemble Unsupervised Machine Learning (ML) algorithm)?
# Isolation Forest is a type of Unsupervised Anomaly Detection Ensemble Machine Learning (ML) algorithm, that consists of
# multiple Isolation Trees ML algorithms which are each trained using a different subset of the dataset and merges them (via Majority 
# Voting, but other types of aggregation methods are possible too) to get a more accurate and stable prediction.

# Step by step of how to Isolation Forest Ensemble ML algorithm detects anomalies:
# - The anomaly score for a data point is derived from the average path length across all trees.
# - A shorter average path length indicates a higher likelihood of being an anomaly.
# - The score is normalized between 0 and 1.

# Isolation Forest is a type of Unsupervised Anomaly Detection Ensemble Machine Learning (ML) algorithm, unlike Random Forest,
# which is a Supervised Classification and Regression Ensemble Machine Learning (ML) algorithm. Isolaiton Forest must be unsupervised, due
# to the nature of anomaly detection tasks. (Since the definition of unsupervised learning is: consists of ML algorithms that are trained 
# using an 'unlabelled dataset', where the data contains only the input parameters. They primarily aim to discover hidden 
# patterns/similarities/clusters within the 'unlabelled dataset') 


# Source(s): 
# - https://www.youtube.com/watch?v=cRzeotaFDwk&t=188s (MachineLearningInterview) (Youtube video titled: 'Anomaly 
#   detection with Isolation Forests')
# - https://www.youtube.com/watch?v=Q7YGBwKVpds (AK Python) (Youtube video titled: 'Machine learning Project: Anomaly detection 
#   using Isolation Forest')

# /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


# thought... wait if HVAC HITL UROP only got an option to pick for a vote up or down, so these independent variables/features is 
# the one on their clothing, ambient temperature, and the various external factors (factors affecting PMV) that Im supposed to 
# decide as part of the 'vector'???


# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
# Random note on the terms 'independent variables/features' and 'dimensions' in Machine Learning (ML):                         /
# These 2 terms are synonymous in the realm of Machine Learning! When a ML model or dataset or problem is said to be of        /
# multi-dimensionality (aka multiple dimensions), what it basically means is that this ML model or dataset or problem considers/
# multiple independent variables/features!                                                                                     /
# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


# The beauty of mathematics is that even though you cannot visualise high dimensions/independent variables/features, but the math 
# will continue to work


# The 'vector' that will be used in the HVAC HITL RL model, that stores a list of independent variables/features that we will 
# be using in the HVAC HITL aircon simulation (e.g. '[time_of_day, user_type, temperature_preference, metabolic_rate, clothing_insulation]') 

# (flexible to changes, since we can just change the independent variables/features in this 'vector' list)
vector = ['Thermal comfort', 'Age', 'Sex', 'Thermal preference', 'Thermal sensation', 'Air temperature (C)']


# Using the pre-made Isolation Forest algorithm available in the Scikit-learn Python ML library
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest

# Reading the 'thermal_comfort_dataset.csv' dataset from the CSV file using the Pandas library
dataset = pd.read_csv('thermal_comfort_dataset.csv')
print(dataset)


# Creating a 'Isolation Forest ML model' class object/instance
# 1. The 'n_estimater' parameter represents the number of Isolation Tree ML algorithms is being used in the Isolation
#    Forest Ensemble ML algorithm
# 2. Max samples is kind of an allocation that is used to for how many data samples will this algorithm take from the 
#    training dataset, here I gave the value auto, that means I just gave as permission to this algorithm, so it can take any 
#    number of samples from the dataset.
# 3. Contamination means percentage of randomness that may be present in our dataset. With the help of this parameter you can 
#    easily find out the anomalies in your dataset.
# 4. Max independent variables/features, means, that since in some dataset there will be lot of independent variables/features may be present, this parameter will allow 
#    you to select how many independent variables/features you need to include while training the algorithm,Here I gave as 1.0 it means it draw single 
#    feature from the dataset and using that feature it will start to train the data with the Isolation ML algorithm
isolation_forest_ML_model = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.2), max_features=1.0)
isolation_forest_ML_model.fit(dataset[['Thermal comfort', 'Age', 'Sex', 'Thermal preference', 'Thermal sensation', 'Air temperature (C)']])

# The 'vars()' function allows us to see all of an class instance/object's attributes
print(vars(isolation_forest_ML_model))


# The 'decision_function()' in an Isolation Forest is a method that computes the anomaly score for each data point. This score 
# indicates how anomalous or normal a data point is

# (With reference to the sections 'What is a Isolation Tree (Anomaly Detection Unsupervised Machine Learning (ML) algorithm)' and 
#  'What is a Isolation Forest (Anomaly Detection Ensemble Unsupervised Machine Learning (ML) algorithm)' above)
# 1. Isolation Forest is an ensemble of Isolation Trees
#    Each tree isolates data points by randomly selecting independent variables/features and split values. The goal is to isolate anomalies, which tend 
#    to have shorter paths in the tree structure. 

# 2. Anomaly Score Calculation:
#    The anomaly score for a data point is derived from the average path length across all the trees in the forest. Shorter average 
#    path lengths indicate higher likelihoods of being an anomaly. This is because anomalies are easier to isolate and thus reach a 
#    leaf node quickly.

# 3. Decision Function:
#    The 'decision_function()' computes the average path length and then normalizes this score to fall between -1 and 1. A score close to 
#    -1 indicates an anomaly, while a score closer to 1 indicates a normal data point. The function essentially maps the path length 
#    to a standardized anomaly score that can be used to rank data points by their anomaly level.

#    Data points with a negative score (values from -1 to 0) are considered to be anomalies, while data points with a positive score 
#    (values from 0 to 1) are considered to be normal (Note: Zero or Near-Zero Scores: Indicate data points that are borderline cases 
#    between normal and anomalous)
dataset['anomalies_scores'] = isolation_forest_ML_model.decision_function(dataset[['Thermal comfort', 'Age', 'Sex', 'Thermal preference', 'Thermal sensation', 'Air temperature (C)']])


# The 'predict()' function in an Isolation Forest is a method that labels whether a data point is an anomaly (labelled by a value of
# -1) or a data point is normal (labelled by a value of 1) , by identifying data points with a negative score (values from -1 to 0) 
# which the 'predict()' function will label as anomalies, while data points with a positive score (values from 0 to 1), which the 
# 'predict()' function will label as normal

#  Note: Zero or Near-Zero Scores: Indicate data points that are borderline cases between normal and anomalous
dataset['anomaly'] = isolation_forest_ML_model.predict(dataset[['Thermal comfort', 'Age', 'Sex', 'Thermal preference', 'Thermal sensation', 'Air temperature (C)']])
print(dataset)



# Source: 
# https://www.youtube.com/watch?v=Q7YGBwKVpds (AK Python) (Youtube video titled: 'Machine learning Project : Anomaly detection using 
# Isolation Forest') (for this Python code of Isolation Forest (Anomaly Detection Ensemble Unsupervised Machine Learning (ML) algorithm))


# //////////////////////////////////////////////////////////////////////////////////////


# Filter out any rows with a value of '-1' in the 'anomaly' column (since those are anomalies)
dataset_filtered = dataset[dataset['anomaly'] != -1]

print(f'Initial datatset:\n{dataset}')
print(f'Filtered datatset:\n{dataset_filtered}')



